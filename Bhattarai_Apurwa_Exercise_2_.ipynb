{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apurwa2024/Apurwa_INFO5731_FaLL2024/blob/main/Bhattarai_Apurwa_Exercise_2_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ],
      "metadata": {
        "id": "FBKvD6O_TY6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your answer here\n",
        "I am extracting the Urban Area, fortune and Population of the largest companies in the world from wikipedia. Here is teh URL: https://en.wikipedia.org/wiki/Fortune_1000"
      ],
      "metadata": {
        "id": "cikVKDXdTbzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ],
      "metadata": {
        "id": "E9RqrlwdTfvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "url = 'https://en.wikipedia.org/wiki/Fortune_1000'\n",
        "page = requests.get(url)\n",
        "soup = BeautifulSoup(page.text, 'html.parser')\n",
        "\n",
        "table = soup.find_all('table', class_='wikitable sortable')[0]\n",
        "\n",
        "headers = [header.text.strip() for header in table.find_all('th')]\n",
        "\n",
        "df = pd.DataFrame(columns=headers)\n",
        "\n",
        "rows = table.find_all('tr')\n",
        "\n",
        "for i, row in enumerate(rows[1:], 1):\n",
        "    data = row.find_all('td')\n",
        "    row_data = [cell.text.strip() for cell in data]\n",
        "\n",
        "    df.loc[len(df)] = row_data\n",
        "\n",
        "    if i == 1000:\n",
        "        break\n",
        "\n",
        "print(df.head(10))\n",
        "\n",
        "df.to_csv('fortune_1000_companies.csv', index=False)\n",
        "\n",
        "print('saved to fortune_1000_companies.csv')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iFQKPpOteup",
        "outputId": "3430f5fe-6cfb-4a68-ac8d-d26717c2fa83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                             Urban area Number of Fortune 1000 companies  \\\n",
            "0             New York—Newark, NY—NJ—CT                              114   \n",
            "1                        Chicago, IL—IN                               62   \n",
            "2  San Jose--San Francisco--Oakland, CA                               55   \n",
            "3                           Houston, TX                               46   \n",
            "4       Dallas—Fort Worth—Arlington, TX                               40   \n",
            "5    Los Angeles—Long Beach—Anaheim, CA                               32   \n",
            "6                  Washington, DC—VA—MD                               30   \n",
            "7                           Atlanta, GA                               27   \n",
            "8           Minneapolis—St. Paul, MN—WI                               26   \n",
            "9             Philadelphia, PA—NJ—DE—MD                               26   \n",
            "\n",
            "  Population (2010 census)  \n",
            "0               18,351,295  \n",
            "1                8,608,208  \n",
            "2                4,945,700  \n",
            "3                4,944,332  \n",
            "4                5,121,892  \n",
            "5               12,150,996  \n",
            "6                4,586,770  \n",
            "7                4,515,419  \n",
            "8                2,915,899  \n",
            "9                5,441,567  \n",
            "saved to fortune_1000_companies.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YaGLbSHHB8Ej",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "00ee6708-e68c-4041-9f3b-90fadf26c204"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-0e4f49560a45>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m21\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"https://dl.acm.org/action/doSearch?AllField=&AfterYear=2014&BeforeYear=2024&startPage={i}&pageSize=50\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Sleep to avoid hitting rate limits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86; rv:123.0) Gecko/20100101 Firefox/123.0\",\n",
        "    \"Accept\": \"text/html\",\n",
        "    \"Connection\": \"Keep-alive\"\n",
        "}\n",
        "\n",
        "# Prepare to store the results\n",
        "titles = []\n",
        "journals = []\n",
        "years = []\n",
        "authors = []\n",
        "abstracts = []\n",
        "\n",
        "# Collect data from 20 pages (each page contains 50 articles)\n",
        "for i in range(1, 21):\n",
        "    url = f\"https://dl.acm.org/action/doSearch?AllField=&AfterYear=2014&BeforeYear=2024&startPage={i}&pageSize=50\"\n",
        "    time.sleep(5)  # Sleep to avoid hitting rate limits\n",
        "    response = requests.get(url, headers=headers)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to retrieve page {i}\")\n",
        "        continue\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    articles = soup.find_all('div', class_='search__item issue-item-container')\n",
        "\n",
        "    for article in articles:\n",
        "        # Title\n",
        "        title_tag = article.find('h5', class_='issue-item__title')\n",
        "        title = title_tag.get_text(strip=True) if title_tag else \"N/A\"\n",
        "        titles.append(title)\n",
        "\n",
        "        # Journal/Conference Name\n",
        "        journal_tag = article.find('span', class_='epub-section__title')\n",
        "        journal = journal_tag.get_text(strip=True) if journal_tag else \"N/A\"\n",
        "        journals.append(journal)\n",
        "\n",
        "        # Year\n",
        "        year_tag = article.find('span', class_='epub-section__year')\n",
        "        year = year_tag.get_text(strip=True) if year_tag else \"N/A\"\n",
        "        years.append(year)\n",
        "\n",
        "        # Authors\n",
        "        authors_tag = article.find('ul', class_='rlist--inline loa comma')\n",
        "        author_names = [a.get_text(strip=True) for a in authors_tag.find_all('a')] if authors_tag else []\n",
        "        authors.append(\", \".join(author_names) if author_names else \"N/A\")\n",
        "\n",
        "        # Abstract\n",
        "        abstract_tag = article.find('div', class_='issue-item__abstract')\n",
        "        abstract = abstract_tag.get_text(strip=True) if abstract_tag else \"N/A\"\n",
        "        abstracts.append(abstract)\n",
        "\n",
        "# Create a DataFrame and save it to CSV\n",
        "df = pd.DataFrame({\n",
        "    'Title': titles,\n",
        "    'Journal/Conference': journals,\n",
        "    'Year': years,\n",
        "    'Authors': authors,\n",
        "    'Abstract': abstracts\n",
        "})\n",
        "\n",
        "df.to_csv('acm_articles.csv', index=False)\n",
        "\n",
        "print(\"Data collection completed. Articles saved to 'acm_articles.csv'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import requests\n",
        "# from bs4 import BeautifulSoup\n",
        "# import time\n",
        "\n",
        "# headers = {\n",
        "#     \"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86; rv:123.0) Gecko/20100101 Firefox/123.0\",\n",
        "#     \"Accept\": \"text/html\",\n",
        "#     \"Connection\": \"Keep-alive\"\n",
        "# }\n",
        "# results=[]\n",
        "# for i in range (1,25):\n",
        "#   url = f\"https://dl.acm.org/action/doSearch?AllFieldvxyz&Epub0ate=%5B20190913+TO+202409132359%5D&queryID+12%2F7584138766&startPage+{i}&pageSize=50\"\n",
        "#   time.sleep(10)\n",
        "#   response = requests.get(url, headers=headers)\n",
        "#   response_result = BeautifulSoup(response.txt, 'html.parser')\n",
        "#   main = response_result.findAll('1I', class_\"search_item-issue-item.container\")\n",
        "#   results.extend(main)\n",
        "\n",
        "#   titles=[]\n",
        "#   journals=[]\n",
        "#   year_published=[]\n",
        "#   authors=[]\n",
        "#   abstract=[]\n",
        "#   for i in results;\n",
        "#      title=i.find(\"h5\", class_=\"issue-item_title\")\n",
        "#      titles.append(title.g et_text(strip=True))\n",
        "\n",
        "#      journal_name=i.find(\"span\", class_=\"epub-section_title\")\n",
        "#       if journal_name:\n",
        "#         journals.append(journal_ name.get_text(Strip=True))\n",
        "#         else:\n",
        "#           journals.append(\"N/A\")\n",
        "\n",
        "\n",
        "#       df = pd.DataFrame()\n",
        "#       'Title': titles;\n",
        "#       'Journals':  journals;\n",
        "\n",
        "# df.to_csv('articles_data.csv', index=false)\n",
        "\n",
        "# def printing_articles(num1, num2):\n",
        "#   for i in range(num1, num2):\n",
        "#     if not titles[i] or not journals[i] or not year_published[i] or not abstract[i]:\n",
        "#       continue\n",
        "#       print(f\"Article (i)\")\n",
        "#       print(f\"Title (titles[i])\")\n",
        "#       print(f\"Journal: (journals[i])\")\n",
        "\n",
        "\n",
        "#       printing_articles(0,501)\n",
        "#       printing_articles(500,999)\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86; rv:123.0) Gecko/20100101 Firefox/123.0\",\n",
        "    \"Accept\": \"text/html\",\n",
        "    \"Connection\": \"Keep-alive\"\n",
        "}\n",
        "\n",
        "# Prepare to store the results\n",
        "titles = []\n",
        "journals = []\n",
        "years = []\n",
        "authors = []\n",
        "abstracts = []\n",
        "\n",
        "# Collect data from 20 pages (each page contains 50 articles)\n",
        "for i in range(1, 21):\n",
        "    url = f\"https://dl.acm.org/action/doSearch?AllField=&AfterYear=2014&BeforeYear=2024&startPage={i}&pageSize=50\"\n",
        "    time.sleep(5)  # Sleep to avoid hitting rate limits\n",
        "    response = requests.get(url, headers=headers)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to retrieve page {i}\")\n",
        "        continue\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    articles = soup.find_all('div', class_='search__item issue-item-container')\n",
        "\n",
        "    for article in articles:\n",
        "        # Title\n",
        "        title_tag = article.find('h5', class_='issue-item__title')\n",
        "        title = title_tag.get_text(strip=True) if title_tag else \"N/A\"\n",
        "        titles.append(title)\n",
        "\n",
        "        # Journal/Conference Name\n",
        "        journal_tag = article.find('span', class_='epub-section__title')\n",
        "        journal = journal_tag.get_text(strip=True) if journal_tag else \"N/A\"\n",
        "        journals.append(journal)\n",
        "\n",
        "        # Year\n",
        "        year_tag = article.find('span', class_='epub-section__year')\n",
        "        year = year_tag.get_text(strip=True) if year_tag else \"N/A\"\n",
        "        years.append(year)\n",
        "\n",
        "        # Authors\n",
        "        authors_tag = article.find('ul', class_='rlist--inline loa comma')\n",
        "        author_names = [a.get_text(strip=True) for a in authors_tag.find_all('a')] if authors_tag else []\n",
        "        authors.append(\", \".join(author_names) if author_names else \"N/A\")\n",
        "\n",
        "        # Abstract\n",
        "        abstract_tag = article.find('div', class_='issue-item__abstract')\n",
        "        abstract = abstract_tag.get_text(strip=True) if abstract_tag else \"N/A\"\n",
        "        abstracts.append(abstract)\n",
        "\n",
        "# Create a DataFrame and save it to CSV\n",
        "df = pd.DataFrame({\n",
        "    'Title': titles,\n",
        "    'Journal/Conference': journals,\n",
        "    'Year': years,\n",
        "    'Authors': authors,\n",
        "    'Abstract': abstracts\n",
        "})\n",
        "\n",
        "df.to_csv('acm_articles.csv', index=False)\n",
        "\n",
        "print(\"Data collection completed. Articles saved to 'acm_articles.csv'\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9dy7Z-_kBVLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtKskTzbCLaU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "cf0d85c6-f9b4-4f47-f469-37b270b416a2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'praw'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-473613717594>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# write your answer here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpraw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Initialize Reddit API\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'praw'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# write your answer here\n",
        "import praw\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize Reddit API\n",
        "reddit = praw.Reddit(\n",
        "    client_id='YOUR_CLIENT_ID',\n",
        "    client_secret='YOUR_CLIENT_SECRET',\n",
        "    user_agent='YOUR_USER_AGENT'\n",
        ")\n",
        "\n",
        "def fetch_reddit_data(subreddit_name, search_query, limit=100):\n",
        "    subreddit = reddit.subreddit(subreddit_name)\n",
        "    posts = subreddit.search(search_query, limit=limit)\n",
        "\n",
        "    data = []\n",
        "    for post in posts:\n",
        "        data.append({\n",
        "            'Title': post.title,\n",
        "            'Score': post.score,\n",
        "            'URL': post.url,\n",
        "            'Num_Comments': post.num_comments,\n",
        "            'Author': str(post.author)  # Convert None to 'None' if the post has no author\n",
        "        })\n",
        "\n",
        "    return data\n",
        "\n",
        "def main():\n",
        "    subreddit_name = 'all'\n",
        "    search_query = 'your search query'\n",
        "    data_limit = 100\n",
        "\n",
        "    print(f'Fetching data from subreddit: {subreddit_name} with query: {search_query}...')\n",
        "    post_data = fetch_reddit_data(subreddit_name, search_query, limit=data_limit)\n",
        "\n",
        "    df = pd.DataFrame(post_data)\n",
        "    df.to_csv('reddit_data.csv', index=False)\n",
        "    print('Data saved to reddit_data.csv')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I57NXsauCec2"
      },
      "outputs": [],
      "source": [
        "# write your answer here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "sZOhks1dXWEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ],
      "metadata": {
        "id": "eqmHVEwaWhbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Write your response here.\n",
        "'''"
      ],
      "metadata": {
        "id": "akAVJn9YBTQT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FBKvD6O_TY6e",
        "E9RqrlwdTfvl",
        "03jb4GZsBkBS",
        "jJDe71iLB616",
        "55W9AMdXCSpV",
        "4ulBZ6yhCi9F",
        "6SmvS7nSfbj8",
        "sZOhks1dXWEe"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}